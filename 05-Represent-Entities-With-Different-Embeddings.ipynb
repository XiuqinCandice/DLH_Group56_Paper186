{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2152,"status":"ok","timestamp":1681850815748,"user":{"displayName":"Xiuqin Gao","userId":"00765965154190972930"},"user_tz":420},"id":"tvTwvOrlwvyW","outputId":"f5bc0d9b-2308-4843-c4da-eb24c9c07694"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1681850817343,"user":{"displayName":"Xiuqin Gao","userId":"00765965154190972930"},"user_tz":420},"id":"xGTAurDETQIW"},"outputs":[],"source":["DATAPATH = \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer/data/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S01QXtUwwdfv"},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","from gensim.models import Word2Vec, FastText\n","import glove\n","from glove import Corpus\n","\n","import collections\n","import gc \n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6oormoyqwdf1"},"outputs":[],"source":["\n","new_notes = pd.read_pickle(os.path.join(DATAPATH, \"ner_df.p\")) # med7\n","w2vec = Word2Vec.load(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer/embeddings/word2vec.model\")\n","fasttext = FastText.load(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer/embeddings/fasttext.model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzJxksN9wdf2"},"outputs":[],"source":["null_index_list = []\n","for i in new_notes.itertuples():\n","    \n","    if len(i.ner) == 0:\n","        null_index_list.append(i.Index)\n","new_notes.drop(null_index_list, inplace=True)\n","# print(len(new_notes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E95rIQpXwdf3"},"outputs":[],"source":["med7_ner_data = {}\n","\n","for ii in new_notes.itertuples():\n","    \n","    p_id = ii.SUBJECT_ID\n","    ind = ii.Index\n","    \n","    try:\n","        new_ner = new_notes.loc[ind].ner\n","    except:\n","        new_ner = []\n","            \n","    unique = set()\n","    new_temp = []\n","    \n","    for j in new_ner:\n","        for k in j:\n","            \n","            unique.add(k[0])\n","            new_temp.append(k)\n","\n","    if p_id in med7_ner_data:\n","        for i in new_temp:\n","            med7_ner_data[p_id].append(i)\n","    else:\n","        med7_ner_data[p_id] = new_temp\n","# print(len(med7_ner_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGO-3Xy-wdf4"},"outputs":[],"source":["pd.to_pickle(med7_ner_data, (os.path.join(DATAPATH, \"new_ner_word_dict.pkl\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3SbPzMowdf5"},"outputs":[],"source":["def mean(a):\n","    return sum(a) / len(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06wOJ1U4wdf6"},"outputs":[],"source":["data_types = [med7_ner_data]\n","data_names = [\"new_ner\"]\n","\n","for data, names in zip(data_types, data_names):\n","    new_word2vec = {}\n","    print(\"w2vec starting..\")\n","    for k,v in data.items():\n","\n","        patient_temp = []\n","        for i in v:\n","            try:\n","                patient_temp.append(w2vec[i[0]])\n","            except:\n","                avg = []\n","                num = 0\n","                temp = []\n","\n","                if len(i[0].split(\" \")) > 1:\n","                    for each_word in i[0].split(\" \"):\n","                        try:\n","                            temp = w2vec[each_word]\n","                            avg.append(temp)\n","                            num += 1\n","                        except:\n","                            pass\n","                    if num == 0: continue\n","                    avg = np.asarray(avg)\n","                    t = np.asarray(list(map(mean, zip(*avg))))\n","                    patient_temp.append(t)\n","        if len(patient_temp) == 0: continue\n","        new_word2vec[k] = patient_temp\n","    # print(len(new_word2vec))\n","    #############################################################################\n","    print(\"fasttext starting..\")\n","        \n","    new_fasttextvec = {}\n","\n","    for k,v in data.items():\n","\n","        patient_temp = []\n","\n","        for i in v:\n","            try:\n","                patient_temp.append(fasttext[i[0]])\n","            except:\n","                pass\n","        if len(patient_temp) == 0: continue\n","        new_fasttextvec[k] = patient_temp\n","    # print(len(new_fasttextvec))\n","    #############################################################################    \n","\n","    print(\"combined starting..\")\n","    new_concatvec = {}\n","\n","    for k,v in data.items():\n","        patient_temp = []\n","    #     if k != 6: continue\n","        for i in v:\n","            w2vec_temp = []\n","            fasttemp = []\n","            try:\n","                w2vec_temp = w2vec[i[0]]\n","            except:\n","                avg = []\n","                num = 0\n","                temp = []\n","\n","                if len(i[0].split(\" \")) > 1:\n","                    for each_word in i[0].split(\" \"):\n","                        try:\n","                            temp = w2vec[each_word]\n","                            avg.append(temp)\n","                            num += 1\n","                        except:\n","                            pass\n","                    if num == 0: \n","                        w2vec_temp = [0] * 100\n","                    else:\n","                        avg = np.asarray(avg)\n","                        w2vec_temp = np.asarray(list(map(mean, zip(*avg))))  \n","                else:\n","                    w2vec_temp = [0] * 100\n","            #print('value for i',i)    \n","            try:\n","                fasttemp = fasttext[i[0]]\n","            except:\n","                fasttemp = [0] * 100\n","            #print('fasttemp {0}, w2vec_temp{1},'.format(fasttemp,w2vec_temp))\n","            appended = np.append(fasttemp, w2vec_temp, 0)\n","            patient_temp.append(appended)\n","        if len(patient_temp) == 0: continue\n","        new_concatvec[k] = patient_temp\n","\n","\n","    print(len(new_word2vec), len(new_fasttextvec), len(new_concatvec))\n","    \n","    pd.to_pickle(new_word2vec, (os.path.join(DATAPATH, \"new_ner_word2vec_dict.pkl\")))\n","    pd.to_pickle(new_fasttextvec, (os.path.join(DATAPATH, \"new_ner_fasttext_dict.pkl\")))\n","    pd.to_pickle(new_concatvec, (os.path.join(DATAPATH, \"new_ner_combined_dict.pkl\")))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvCenPzdDk7D"},"outputs":[],"source":["new_word2vec_dict = pd.read_pickle(os.path.join(DATAPATH, \"new_ner_word2vec_dict.pkl\"))\n","new_fasttext_dict = pd.read_pickle(os.path.join(DATAPATH, \"new_ner_fasttext_dict.pkl\"))\n","new_combined_dict = pd.read_pickle(os.path.join(DATAPATH, \"new_ner_combined_dict.pkl\"))\n","\n","diff = set(new_fasttext_dict.keys()).difference(set(new_word2vec_dict))\n","for i in diff:\n","    del new_fasttext_dict[i]\n","    del new_combined_dict[i]\n","print (len(new_word2vec_dict), len(new_fasttext_dict), len(new_combined_dict))\n","\n","pd.to_pickle(new_word2vec_dict, (os.path.join(DATAPATH, \"new_ner_word2vec_limited_dict.pkl\")))\n","pd.to_pickle(new_fasttext_dict, (os.path.join(DATAPATH, \"new_ner_fasttext_limited_dict.pkl\")))\n","pd.to_pickle(new_combined_dict, (os.path.join(DATAPATH, \"new_ner_combined_limited_dict.pkl\")))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1RaRHM1cJhL79J2XMAFDOKmz_p2dRCuWP","timestamp":1651206739602},{"file_id":"1q77LzBKaSFNVxY4KJGCIuABuCbz0qkJL","timestamp":1650732034329},{"file_id":"1p7XCSF22Xv0rAUBi5XshXtqDhowvu4ks","timestamp":1649799600818},{"file_id":"1OAgfEwQSo5jSse74gsIH4cv3aVD3RVIw","timestamp":1649619327189}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
