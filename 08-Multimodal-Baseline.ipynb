{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"49fsvRy3ehFi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681851458643,"user_tz":420,"elapsed":24994,"user":{"displayName":"Xiuqin Gao","userId":"00765965154190972930"}},"outputId":"5c017902-b330-44b5-d21d-eeeb78a51ab9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NX-XlerAdtTa"},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","#from gensim.models import Word2Vec, FastText\n","#import glove\n","#from glove import Corpus\n","\n","import collections\n","import gc \n","\n","import keras\n","from keras import backend as K\n","from keras import regularizers\n","from keras.models import Sequential, Model\n","from keras.layers import Flatten, Dense, Dropout, Input, concatenate, merge, Activation, Concatenate, LSTM, GRU\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n","from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D, merge\n","\n","#from keras.optimizers import Adam\n","\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n","from keras.utils import np_utils\n","#from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n","from keras.backend import set_session, clear_session, get_session\n","import tensorflow as tf\n","\n","\n","from sklearn.utils import class_weight\n","from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["# Reset Keras Session\n","def reset_keras(model):\n","    sess = get_session()\n","    clear_session()\n","    sess.close()\n","    sess = get_session()\n","\n","    try:\n","        del model # this is from global space - change this as you need\n","    except:\n","        pass\n","\n","    gc.collect() # if it's done something you should see a number being outputted"],"metadata":{"id":"-aIAOUBRGXEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jimBrnTCdtTx"},"outputs":[],"source":["def create_dataset(dict_of_ner):\n","    temp_data = []\n","    for k, v in sorted(dict_of_ner.items()):\n","        temp = []\n","        for embed in v:\n","            temp.append(embed)\n","        temp_data.append(np.mean(temp, axis = 0)) \n","    return np.asarray(temp_data)\n","\n","def make_prediction_multi_avg(model, test_data):\n","    probs = model.predict(test_data)\n","    y_pred = [1 if i>=0.5 else 0 for i in probs]\n","    return probs, y_pred\n","\n","def save_scores_multi_avg(predictions, probs, ground_truth, \n","                          \n","                          embed_name, problem_type, iteration, hidden_unit_size,\n","                          \n","                          sequence_name, type_of_ner):\n","    \n","    auc = roc_auc_score(ground_truth, probs)\n","    auprc = average_precision_score(ground_truth, probs)\n","    acc   = accuracy_score(ground_truth, predictions)\n","    F1    = f1_score(ground_truth, predictions)\n","    \n","    result_dict = {}    \n","    result_dict['auc'] = auc\n","    result_dict['auprc'] = auprc\n","    result_dict['acc'] = acc\n","    result_dict['F1'] = F1\n","    \n","    result_path = \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/results/\"\n","    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n","    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-avg-.p\"\n","    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n","\n","    print(auc, auprc, acc, F1)\n","    \n","def avg_ner_model(layer_name, number_of_unit, embedding_name):\n","\n","    if embedding_name == \"concat\":\n","        input_dimension = 200\n","        #print(\"input_dimension\", input_dimension)\n","    else:\n","        input_dimension = 100\n","    input_dimension = 100\n","    sequence_input = Input(shape=(24,104))\n","\n","    input_avg = Input(shape=(input_dimension, ), name = \"avg\")        \n","#     x_1 = Dense(256, activation='relu')(input_avg)\n","#     x_1 = Dropout(0.3)(x_1)\n","    \n","    if layer_name == \"GRU\":\n","        x = GRU(number_of_unit)(sequence_input)\n","    elif layer_name == \"LSTM\":\n","        x = LSTM(number_of_unit)(sequence_input)\n","\n","    x = keras.layers.Concatenate()([x, input_avg])\n","\n","    x = Dense(256, activation='relu')(x)\n","    x = Dropout(0.2)(x)\n","    \n","    \n","    #logits_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n","    logits_regularizer = tf.keras.regularizers.L2(0.01)\n","    preds = Dense(1, activation='sigmoid',use_bias=False,\n","                         kernel_initializer=tf.keras.initializers.glorot_normal(), \n","                  kernel_regularizer=logits_regularizer)(x)\n","    \n","    \n","    #opt = Adam(lr=0.001, decay = 0.01)\n","    model = Model(inputs=[sequence_input, input_avg], outputs=preds)\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer=\"adam\",\n","                  metrics=['acc'])\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVpt6AQ2EGdS"},"outputs":[],"source":["lvl2_train =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/lvl2_imputer_train_los.pkl\")\n","lvl2_dev =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/lvl2_imputer_dev_los.pkl\")\n","lvl2_test =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/lvl2_imputer_test_los.pkl\")\n","\n","Ys =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_los.pkl\")\n","Ys_train =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_train_los.pkl\")\n","Ys_dev =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_dev_los.pkl\")\n","Ys_test =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_test_los.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxy2Bi0aEGdT"},"outputs":[],"source":["all_train_ids = set()\n","for i in Ys_train.itertuples():\n","    all_train_ids.add( i.Index[0] )\n","    \n","all_dev_ids = set()\n","for i in Ys_dev.itertuples():\n","    all_dev_ids.add( i.Index[0] )\n","    \n","all_test_ids = set()\n","for i in Ys_test.itertuples():\n","    all_test_ids.add( i.Index[0] )\n","\n","print (sum(Ys_train.mort_icu.values)*1.0 / len(Ys_train.mort_icu.values))\n","print (sum(Ys_dev.mort_icu.values)*1.0 / len(Ys_dev.mort_icu.values))\n","print (sum(Ys_test.mort_icu.values)*1.0 / len(Ys_test.mort_icu.values))\n","print (\"====\")\n","print (sum(Ys_train.mort_hosp.values)*1.0 / len(Ys_train.mort_hosp.values))\n","print (sum(Ys_dev.mort_hosp.values)*1.0 / len(Ys_dev.mort_hosp.values))\n","print (sum(Ys_test.mort_hosp.values)*1.0 / len(Ys_test.mort_hosp.values))\n","print (\"====\")\n","print (sum(Ys_train.los_3.values)*1.0 / len(Ys_train.los_3.values))\n","print (sum(Ys_dev.los_3.values)*1.0 / len(Ys_dev.los_3.values))\n","print (sum(Ys_test.los_3.values)*1.0 / len(Ys_test.los_3.values))\n","print (\"====\")\n","print (sum(Ys_train.los_7.values)*1.0 / len(Ys_train.los_7.values))\n","print (sum(Ys_dev.los_7.values)*1.0 / len(Ys_dev.los_7.values))\n","print (sum(Ys_test.los_7.values)*1.0 / len(Ys_test.los_7.values))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UWuADlkdtUA"},"outputs":[],"source":["type_of_ner = \"new\"\n","\n","x_train_lstm = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_x_train_los.pkl\")\n","x_dev_lstm = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_x_dev_los.pkl\")\n","x_test_lstm = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_x_test_los.pkl\")\n","\n","y_train = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_y_train_los.pkl\")\n","y_dev = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_y_dev_los.pkl\")\n","y_test = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_y_test_los.pkl\")\n","\n","ner_word2vec = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner_word2vec_limited_dict_los.pkl\")\n","ner_fasttext = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner_fasttext_limited_dict_los.pkl\")\n","ner_concat = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner_combined_limited_dict_los.pkl\")\n","\n","new_keys = set(ner_word2vec.keys())\n","train_ids = sorted(all_train_ids.intersection(new_keys))\n","dev_ids = sorted(all_dev_ids.intersection(new_keys))\n","test_ids = sorted(all_test_ids.intersection(new_keys))\n","\n","#train_ids = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_train_ids.pkl\")\n","#dev_ids = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_dev_ids.pkl\")\n","#test_ids = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_test_ids.pkl\")\n","\n","print(\"train_ids = \", train_ids)\n","print(\"dev_ids = \", dev_ids)\n","print(\"test_ids = \", test_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ihm-zPxBdtUF"},"outputs":[],"source":["#embedding_types = ['concat']\n","#embedding_dict = [ner_concat]\n","#target_problems = ['mort_hosp']\n","\n","embedding_types = ['word2vec', 'fasttext', 'concat']\n","embedding_dict = [ner_word2vec, ner_fasttext, ner_concat]\n","target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_5', 'los_7']\n","\n","\n","#num_epoch = 2\n","num_epoch = 100\n","model_patience = 3\n","monitor_criteria = 'val_loss'\n","batch_size = 64\n","iter_num = 11\n","#iter_num = 2\n","unit_sizes = [128, 256]\n","#unit_sizes = [256]\n","#unit_sizes = [128]\n","#layers = [\"LSTM\", \"GRU\"]\n","layers = [\"GRU\"]\n","\n","for each_layer in layers:\n","    print (\"Layer: \", each_layer)\n","    for each_unit_size in unit_sizes:\n","        print (\"Hidden unit: \", each_unit_size)\n","\n","        for embed_dict, embed_name in zip(embedding_dict, embedding_types):    \n","            print (\"Embedding: \", embed_name)\n","            print(\"=============================\")\n","\n","            temp_train_ner = dict((k, ner_word2vec[k]) for k in train_ids)\n","            temp_dev_ner = dict((k, ner_word2vec[k]) for k in dev_ids)\n","            temp_test_ner = dict((k, ner_word2vec[k]) for k in test_ids)\n","\n","            #print(\"temp_train_ner: \", temp_train_ner.shape)\n","            #print(\"temp_dev_ner: \", temp_dev_ner.shape)\n","            #print(\"temp_test_ner: \", temp_test_ner.shape)\n","\n","            x_train_ner = create_dataset(temp_train_ner)\n","            x_dev_ner = create_dataset(temp_dev_ner)\n","            x_test_ner = create_dataset(temp_test_ner)\n","\n","            for iteration in range(1, iter_num):\n","                print (\"Iteration number: \", iteration)\n","\n","                for each_problem in target_problems:\n","                    print (\"Problem type: \", each_problem)\n","                    print (\"__________________\")\n","\n","                    early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n","                    best_model_name = \"avg-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n","                    checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,\n","                        save_best_only=True, mode='min', period=1)\n","\n","\n","                    callbacks = [early_stopping_monitor, checkpoint]\n","\n","                    model = avg_ner_model(each_layer, each_unit_size, embed_name)\n","                    \n","                    #print(\"x_train_lstm: \", x_train_lstm.shape)\n","                    #print(\"x_train_ner: \", x_train_ner.shape)\n","                    #print(\"y_train[each_problem]: \", (y_train[each_problem]).shape)\n","                    #print(\"x_dev_lstm: \", x_dev_lstm.shape)\n","                    #print(\"x_dev_ner: \", x_dev_ner.shape)\n","                    #print(\"y_dev[each_problem]: \", (y_dev[each_problem]).shape)\n","\n","\n","                    model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1, \n","                              validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, \n","                              batch_size=batch_size )\n","\n","                    model.load_weights(best_model_name)\n","\n","                    probs, predictions = make_prediction_multi_avg(model, [x_test_lstm, x_test_ner])\n","                    \n","                    save_scores_multi_avg(predictions, probs, y_test[each_problem], \n","                                embed_name, each_problem, iteration, each_unit_size, \n","                                each_layer, type_of_ner)\n","                    \n","                    reset_keras(model)\n","                    #del model\n","                    clear_session()\n","                    gc.collect()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[{"file_id":"1UVCwqEa9oBTs454oPvT02Bolfu_jYiqr","timestamp":1651118759028}],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}